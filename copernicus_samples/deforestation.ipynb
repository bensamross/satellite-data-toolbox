{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5e6d71",
   "metadata": {},
   "source": [
    "# Deforestation Monitoring using Sentinel 2 and xarray\n",
    "Sentinel 2 data is one of the most popular satellite datasets, but it does come with challenges. Cloud-free mosaics have to be constructed often in order to get analysis-ready data. Accessing a lot of data through tiles takes a long time, and getting the data into a format it can be easily analysed in with common Python tools can be a challenge.\n",
    "\n",
    "In this notebook, we will show how this whole process of getting analysis-ready data into Python can be sped up by using the Copernicus Dataspace Ecosystem and Sentinel Hub APIs. This is being presented by running through a basic deforestation monitoring use-case. The notebook uses the popular xarray Python library to handle the multidimensional data.\n",
    "\n",
    "What we show in this notebook:\n",
    "\n",
    "- How to access Sentinel 2 data in the Copernicus Dataspace Ecosystem\n",
    "- Calculation of NDVI in the Cloud\n",
    "- Monthly composites\n",
    "- Creating a time series\n",
    "- Loading data into xarray\n",
    "- Basic classification using thresholding\n",
    "- Accuracy assessment of classification\n",
    "\n",
    "## Prerequisites\n",
    "- A Copernicus Dataspace Ecosystem account\n",
    "- Basic understanding of the Sentinel Hub Processing API (Introductory Notebook available here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74f22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from ipyleaflet import GeoJSON, Map, basemaps\n",
    "from sentinelhub import (\n",
    "    CRS,\n",
    "    BBox,\n",
    "    DataCollection,\n",
    "    MimeType,\n",
    "    SentinelHubDownloadClient,\n",
    "    SentinelHubRequest,\n",
    "    SHConfig,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "# scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d9b8d",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "To obtain your client_id & client_secret, you need to navigate to your Dashboard. In the User Settings, you can create a new OAuth client to generate these credentials. More detailed instructions can be found on the corresponding documentation page.\n",
    "\n",
    "Now that you have your client_id & client_secret, it is recommended to configure a new profile in your Sentinel Hub Python package. Instructions on how to configure your Sentinel Hub Python package can be found here. Following these instructions, you can create a profile specifically for using the package to access Copernicus Data Space Ecosystem data collections. This is useful as changes to the config class in your notebook are usually only temporary and by saving the configuration to your profile, you don’t have to generate new credentials or overwrite/change the default profile every time you run or write a new Jupyter Notebook.\n",
    "\n",
    "If you are using the Sentinel Hub Python package for the Copernicus Data Space Ecosystem for the first time, you should create a profile specifically for the Copernicus Data Space Ecosystem. You can do this in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b782267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you have not created a configuration.\n",
    "\n",
    "config = SHConfig()\n",
    "# config.sh_client_id = getpass.getpass(\"sh-f9c33c40-7ac5-43e2-8304-582267c9ab83\")\n",
    "# config.sh_client_secret = getpass.getpass(\"9luiFOQDgS3Jj2qsSz6qWOIRMOVUPsSf\")\n",
    "config.sh_client_id = \"sh-f9c33c40-7ac5-43e2-8304-582267c9ab83\"\n",
    "config.sh_client_secret = \"9luiFOQDgS3Jj2qsSz6qWOIRMOVUPsSf\"\n",
    "config.sh_base_url = \"https://sh.dataspace.copernicus.eu\"\n",
    "config.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "# config.save(\"cdse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ac6ff",
   "metadata": {},
   "source": [
    "However, if you have already configured a profile in Sentinel Hub Python for the Copernicus Data Space Ecosystem, then you can run the below cell entering the profile name as a string replacing profile_name.\n",
    "\n",
    "\n",
    "```python\n",
    "clientName = 'python'\n",
    "clientId = 'sh-f9c33c40-7ac5-43e2-8304-582267c9ab83'\n",
    "clientSecret = '9luiFOQDgS3Jj2qsSz6qWOIRMOVUPsSf'\n",
    "expiry = '2025-07-07'\n",
    "```\n",
    "\n",
    "```bash\n",
    "!sentinelhub.config --sh_client_secret 9luiFOQDgS3Jj2qsSz6qWOIRMOVUPsSf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f17fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = SHConfig(\"profile-name\")\n",
    "config = SHConfig(\"default-profile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d717c",
   "metadata": {},
   "source": [
    "## Area of Interest\n",
    "First, we define an area of interest. In this case the area of interest is in the Harz Mountains in Germany since we are aware of substantial forest dieback in recent years.\n",
    "\n",
    "The resolution is defined in the units of the coordinate reference system. Because we want to define units in meters, we also need to define the bounding box coordinates in a CRS using meters. We use EPSG:3035 in this case. This CRS is only available for Europe, outside of Europe we could use EPSG:3857 or UTM Zones.\n",
    "\n",
    "You can also explore the area of interest in the Copernicus Browser here.\n",
    "\n",
    "![https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/img/deforestation_thumbnail.jpg](https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/img/deforestation_thumbnail.jpg)\n",
    "\n",
    "Deforestation in Harz Mountain as seen from Sentinel-2, June 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f2a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired resolution of our data\n",
    "resolution = (100, 100)\n",
    "bbox_coords = [10.633501, 51.611195, 10.787234, 51.698098]\n",
    "epsg = 3035\n",
    "# Convert to 3035 to get crs with meters as units\n",
    "bbox = BBox(bbox_coords, CRS(4326)).transform(epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40648ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634eaf7f72724df49d2284a3b5604618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[51.654646497292276, 10.710367500000002], controls=(ZoomControl(options=['position', 'zoom_in_text'…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = bbox.transform(4326).middle\n",
    "\n",
    "# Add OSM background\n",
    "overview_map = Map(basemap=basemaps.OpenStreetMap.Mapnik, center=(y, x), zoom=10)\n",
    "\n",
    "# Add geojson data\n",
    "geo_json = GeoJSON(data=bbox.transform(4326).geojson)\n",
    "overview_map.add_layer(geo_json)\n",
    "\n",
    "# Display\n",
    "overview_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3419909",
   "metadata": {},
   "source": [
    "## Data Access\n",
    "Next, we define our evalscript. The evalscript is a piece of JavaScript code that tells the Copernicus Dataspace Ecosystem how to process the pixels you request before they are delivered to you.\n",
    "\n",
    "This makes it a very powerful tool to perform pixel-based calculations in the cloud. For inspiration on what can be done in an evalscript, there is an extensive online resource of community-created evalscripts called custom-scripts. In this example, we want to calculate cloud-free mosaics. This is a perfect application for an evalscript, as you do not have to download the data needed to generate the mosaic, but all calculations are done on the server and only the final cloud-free mosaic is delivered.\n",
    "\n",
    "So let’s go over how this is done.\n",
    "\n",
    "The evalscript needs to define two functions, setup() and evaluatePixel(). First, let’s look at the setup function:\n",
    "\n",
    "```javascript\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"B08\", \"B04\", \"B03\", \"B02\", \"SCL\"],\n",
    "        output: {\n",
    "            bands: 5,\n",
    "            sampleType: \"INT16\"\n",
    "        },\n",
    "        mosaicking: \"ORBIT\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Here we specify which bands we want to request. In this case, we get the bands needed to calculate the NDVI and to display a True Color Image. We also define how our output should be structured, and define the output as a 5-band image with the INT16 data type.\n",
    "\n",
    "Finally, we specify the mosaicking parameter. This determines how the pixel values are returned to us. - mosaicking: \"SIMPLE\" returns only a single pixel, either from the most recent, the least recent or the least cloudy Sentinel 2 tile.\n",
    "\n",
    "- mosaicking: \"ORBIT\" returns all pixels of unique orbits for the entire time series as a list. We use this to obtain all possible values from which we can create the cloud-free mosaic.\n",
    "\n",
    "Next let’s take a look at the evaluatePixel() function. This is the function where the actual calculation is defined:\n",
    "\n",
    "```javascript\n",
    "function evaluatePixel(samples) {\n",
    "    var valid = samples.filter(validate);\n",
    "    if (valid.length > 0 ) {\n",
    "        let cloudless = {\n",
    "            b08: getFirstQuartileValue(valid.map(s => s.B08)),\n",
    "            b04: getFirstQuartileValue(valid.map(s => s.B04)),\n",
    "            b03: getFirstQuartileValue(valid.map(s => s.B03)),\n",
    "            b02: getFirstQuartileValue(valid.map(s => s.B02)),\n",
    "        }\n",
    "        let ndvi = ((cloudless.b08 - cloudless.b04) / (cloudless.b08 + cloudless.b04))\n",
    "        // This applies a scale factor so the data can be saved as an int\n",
    "        let scale = [cloudless.b04, cloudless.b03, cloudless.b02, ndvi].map(v => v*10000);\n",
    "        return scale\n",
    "    }\n",
    "    // If there isn't enough data, return NODATA\n",
    "    return [-32768, -32768, -32768, -32768]\n",
    "}\n",
    "```\n",
    "\n",
    "The way we construct the cloud free mosaic is by first filtering all the available acquisitions to only include the ones which contain clear data with samples.filter(validate);. Then we sort the array and get the value at the first quartile of the array. Getting the first quartile instead of the mean or median further reduces the risk that we select a cloudy pixel.\n",
    "\n",
    "Finally, we calculate the NDVI using the cloud-free values and return all the desired values as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a54856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript_cloudless = \"\"\"\n",
    "    //VERSION=3\n",
    "    function setup() {\n",
    "        return {\n",
    "            input: [\"B08\", \"B04\", \"B03\", \"B02\", \"SCL\"],\n",
    "            output: {\n",
    "                bands: 4,\n",
    "                sampleType: \"INT16\"\n",
    "            },\n",
    "            mosaicking: \"ORBIT\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    function getFirstQuartileValue(values) {\n",
    "        values.sort((a,b) => a-b);\n",
    "        return getFirstQuartile(values);\n",
    "    }\n",
    "\n",
    "    function getFirstQuartile(sortedValues) {\n",
    "        var index = Math.floor(sortedValues.length / 4);\n",
    "        return sortedValues[index];\n",
    "    }\n",
    "\n",
    "    function validate(sample) {\n",
    "        // Define codes as invalid:\n",
    "        const invalid = [\n",
    "            0, // NO_DATA\n",
    "            1, // SATURATED_DEFECTIVE\n",
    "            3, // CLOUD_SHADOW\n",
    "            7, // CLOUD_LOW_PROBA\n",
    "            8, // CLOUD_MEDIUM_PROBA\n",
    "            9, // CLOUD_HIGH_PROBA\n",
    "            10 // THIN_CIRRUS\n",
    "        ]\n",
    "        return !invalid.includes(sample.SCL)\n",
    "    }\n",
    "\n",
    "    function evaluatePixel(samples) {\n",
    "        var valid = samples.filter(validate);\n",
    "        if (valid.length > 0 ) {\n",
    "            let cloudless = {\n",
    "                b08: getFirstQuartileValue(valid.map(s => s.B08)),\n",
    "                b04: getFirstQuartileValue(valid.map(s => s.B04)),\n",
    "                b03: getFirstQuartileValue(valid.map(s => s.B03)),\n",
    "                b02: getFirstQuartileValue(valid.map(s => s.B02)),\n",
    "            }\n",
    "            let ndvi = ((cloudless.b08 - cloudless.b04) / (cloudless.b08 + cloudless.b04))\n",
    "            // This applies a scale factor so the data can be saved as an int\n",
    "            let scale = [cloudless.b04, cloudless.b03, cloudless.b02, ndvi].map(v => v*10000);\n",
    "            return scale\n",
    "        }\n",
    "        // If there isn't enough data, return NODATA\n",
    "        return [-32768, -32768, -32768, -32768]\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641445f",
   "metadata": {},
   "source": [
    "We have defined how the pixels should be handled. However, we still need to define some other parameters to get a full request.\n",
    "\n",
    "We need to define which data we want to use and the timeframe of the data.\n",
    "\n",
    "This is what we are doing in the next cell. Here, we also start building our time series. To see changes over the years, we want to get cloud-free mosaics for the same 3 months over the years. We do this by defining the three months (June-August) in the interval_of_interest() function. Then we define a function get_request(), which will build the request to the Sentinel Hub API on the Copernicus Data Space Ecosystem.\n",
    "\n",
    "In this SentinelHubRequest, we define the input data, the timeframe, the output type (TIFF), the bounding box, the resolution and where to save the data.\n",
    "\n",
    "We define this as a function because we want to make several requests with the changing years being the only input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d4beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_of_interest(year):\n",
    "    return (datetime(year, 6, 1), datetime(year, 9, 1))\n",
    "\n",
    "\n",
    "def get_request(year):\n",
    "    time_interval = interval_of_interest(year)\n",
    "    return SentinelHubRequest(\n",
    "        evalscript=evalscript_cloudless,\n",
    "        input_data=[\n",
    "            SentinelHubRequest.input_data(\n",
    "                data_collection=DataCollection.SENTINEL2_L2A.define_from(\n",
    "                    \"s2\", service_url=config.sh_base_url\n",
    "                ),\n",
    "                time_interval=time_interval,\n",
    "            )\n",
    "        ],\n",
    "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "        bbox=bbox,\n",
    "        resolution=resolution,\n",
    "        config=config,\n",
    "        data_folder=\"./data\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983c87a",
   "metadata": {},
   "source": [
    "This cell now creates a request for each of the years, from 2018 to 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a467d3a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data collection definition is already taken by DataCollection.SENTINEL2_L2A. Two different DataCollection enums cannot have the same definition.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m sh_requests = {}\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2018\u001b[39m, \u001b[32m2024\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     sh_requests[year] = \u001b[43mget_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m sh_requests\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mget_request\u001b[39m\u001b[34m(year)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_request\u001b[39m(year):\n\u001b[32m      6\u001b[39m     time_interval = interval_of_interest(year)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SentinelHubRequest(\n\u001b[32m      8\u001b[39m         evalscript=evalscript_cloudless,\n\u001b[32m      9\u001b[39m         input_data=[\n\u001b[32m     10\u001b[39m             SentinelHubRequest.input_data(\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m                 data_collection=\u001b[43mDataCollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSENTINEL2_L2A\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefine_from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msh_base_url\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     14\u001b[39m                 time_interval=time_interval,\n\u001b[32m     15\u001b[39m             )\n\u001b[32m     16\u001b[39m         ],\n\u001b[32m     17\u001b[39m         responses=[SentinelHubRequest.output_response(\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, MimeType.TIFF)],\n\u001b[32m     18\u001b[39m         bbox=bbox,\n\u001b[32m     19\u001b[39m         resolution=resolution,\n\u001b[32m     20\u001b[39m         config=config,\n\u001b[32m     21\u001b[39m         data_folder=\u001b[33m\"\u001b[39m\u001b[33m./data\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rossb\\AppData\\Local\\anaconda3\\envs\\py312\\Lib\\site-packages\\sentinelhub\\data_collections.py:500\u001b[39m, in \u001b[36mDataCollection.define_from\u001b[39m\u001b[34m(self, name, **params)\u001b[39m\n\u001b[32m    497\u001b[39m definition = \u001b[38;5;28mself\u001b[39m.value\n\u001b[32m    498\u001b[39m new_definition = definition.derive(**params, _name=name)\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_add_data_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_definition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataCollection(new_definition)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rossb\\AppData\\Local\\anaconda3\\envs\\py312\\Lib\\site-packages\\sentinelhub\\data_collections.py:524\u001b[39m, in \u001b[36mDataCollection._try_add_data_collection\u001b[39m\u001b[34m(cls, name, definition)\u001b[39m\n\u001b[32m    521\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData collection name `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` is already taken by another data collection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    523\u001b[39m existing_collection = \u001b[38;5;28mcls\u001b[39m._value2member_map_[definition]\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    525\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData collection definition is already taken by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisting_collection\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Two different \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDataCollection enums cannot have the same definition.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Data collection definition is already taken by DataCollection.SENTINEL2_L2A. Two different DataCollection enums cannot have the same definition."
     ]
    }
   ],
   "source": [
    "# create a dictionary of requests\n",
    "sh_requests = {}\n",
    "for year in range(2018, 2024):\n",
    "    sh_requests[year] = get_request(year)\n",
    "\n",
    "sh_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95457220",
   "metadata": {},
   "source": [
    "The next step is to download the data. This is done with the utility function SentinelHubDownloadClient. It downloads a list of requests in parallel, greatly improving the download speed. Before we can do that, we need to change the format of the requests slightly, which is done in the variable list_of_requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2d90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_requests = [request.download_list[0] for request in sh_requests.values()]\n",
    "\n",
    "# download data with multiple threads\n",
    "data = SentinelHubDownloadClient(config=config).download(\n",
    "    list_of_requests, max_threads=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ce76d",
   "metadata": {},
   "source": [
    "The output of the requests do not provide any information about which year the data is from, so we rename the output of each request to the year of the data it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c8c1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_output_path(request):\n",
    "    # Gets the full path to the output from a request\n",
    "    return Path(request.data_folder, request.get_filename_list()[0])\n",
    "\n",
    "\n",
    "# Moves and renames the files to the root directory of results\n",
    "for year, request in sh_requests.items():\n",
    "    request_output_path(request).rename(f\"./data/{year}.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb945f80",
   "metadata": {},
   "source": [
    "## Read data with xarray\n",
    "Now we can load the data into xarray. We use rioxarray, an extension for xarray, to load multiple tiffs into a single xarray dataset. xarray is a scalable tool for analysing multidimensional data in Python. This makes xarray ideal for analysing time series data.\n",
    "\n",
    "The different files correspond to the time dimension, but xarray does not know which file is which time step. Therefore, we add a pre-processing step in which we parse out the year from the filename and add it as the time dimension for that file.\n",
    "\n",
    "The warnings in the output can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8efae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_dim(xda):\n",
    "    # This pre-processes the file to add the correct\n",
    "    # year from the filename as the time dimension\n",
    "    year = int(Path(xda.encoding[\"source\"]).stem)\n",
    "    return xda.expand_dims(year=[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8345954",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unrecognized engine 'rasterio' must be one of your download engines: ['scipy', 'store']. To install additional dependencies, see:\nhttps://docs.xarray.dev/en/stable/user-guide/io.html \nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tiff_paths = Path(\u001b[33m\"\u001b[39m\u001b[33m./data\u001b[39m\u001b[33m\"\u001b[39m).glob(\u001b[33m\"\u001b[39m\u001b[33m*.tif\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ds_s2 = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtiff_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrasterio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_time_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mband_as_variable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m ds_s2 = ds_s2.rename(\n\u001b[32m      9\u001b[39m     {\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mband_1\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     }\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m ds_s2 = ds_s2 / \u001b[32m10000\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rossb\\AppData\\Local\\anaconda3\\envs\\py312\\Lib\\site-packages\\xarray\\backends\\api.py:1635\u001b[39m, in \u001b[36mopen_mfdataset\u001b[39m\u001b[34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[39m\n\u001b[32m   1632\u001b[39m     open_ = open_dataset\n\u001b[32m   1633\u001b[39m     getattr_ = \u001b[38;5;28mgetattr\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1635\u001b[39m datasets = [\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths1d]\n\u001b[32m   1636\u001b[39m closers = [getattr_(ds, \u001b[33m\"\u001b[39m\u001b[33m_close\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[32m   1637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rossb\\AppData\\Local\\anaconda3\\envs\\py312\\Lib\\site-packages\\xarray\\backends\\api.py:673\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    671\u001b[39m     from_array_kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m backend = \u001b[43mplugins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m decoders = _resolve_decoders_kwargs(\n\u001b[32m    676\u001b[39m     decode_cf,\n\u001b[32m    677\u001b[39m     open_backend_dataset_parameters=backend.open_dataset_parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    683\u001b[39m     decode_coords=decode_coords,\n\u001b[32m    684\u001b[39m )\n\u001b[32m    686\u001b[39m overwrite_encoded_chunks = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33moverwrite_encoded_chunks\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rossb\\AppData\\Local\\anaconda3\\envs\\py312\\Lib\\site-packages\\xarray\\backends\\plugins.py:202\u001b[39m, in \u001b[36mget_backend\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m    200\u001b[39m     engines = list_engines()\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m engines:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    203\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33munrecognized engine \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m must be one of your download engines: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(engines)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTo install additional dependencies, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         )\n\u001b[32m    208\u001b[39m     backend = engines[engine]\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(engine, BackendEntrypoint):\n",
      "\u001b[31mValueError\u001b[39m: unrecognized engine 'rasterio' must be one of your download engines: ['scipy', 'store']. To install additional dependencies, see:\nhttps://docs.xarray.dev/en/stable/user-guide/io.html \nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html"
     ]
    }
   ],
   "source": [
    "tiff_paths = Path(\"./data\").glob(\"*.tif\")\n",
    "ds_s2 = xr.open_mfdataset(\n",
    "    tiff_paths,\n",
    "    engine=\"rasterio\",\n",
    "    preprocess=add_time_dim,\n",
    "    band_as_variable=True,\n",
    ")\n",
    "ds_s2 = ds_s2.rename(\n",
    "    {\n",
    "        \"band_1\": \"R\",\n",
    "        \"band_2\": \"G\",\n",
    "        \"band_3\": \"B\",\n",
    "        \"band_4\": \"NDVI\",\n",
    "    }\n",
    ")\n",
    "ds_s2 = ds_s2 / 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec1711",
   "metadata": {},
   "source": [
    "We can use the resultatnt xarray to plot the RGB data as a true color image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc03058a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_s2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get RGB data for a year\u001b[39;00m\n\u001b[32m      2\u001b[39m plot_year = \u001b[32m2018\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m true_color = \u001b[43mds_s2\u001b[49m.sel(year=plot_year)[[\u001b[33m\"\u001b[39m\u001b[33mR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mG\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m]].to_array()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Divide by scale factor and apply gamma to brighten image\u001b[39;00m\n\u001b[32m      5\u001b[39m (true_color * \u001b[32m4\u001b[39m).plot.imshow()\n",
      "\u001b[31mNameError\u001b[39m: name 'ds_s2' is not defined"
     ]
    }
   ],
   "source": [
    "# Get RGB data for a year\n",
    "plot_year = 2018\n",
    "true_color = ds_s2.sel(year=plot_year)[[\"R\", \"G\", \"B\"]].to_array()\n",
    "# Divide by scale factor and apply gamma to brighten image\n",
    "(true_color * 4).plot.imshow()\n",
    "plt.title(f\"True Color {plot_year}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe98951",
   "metadata": {},
   "source": [
    "![https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-14-output-1.png](https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-14-output-1.png)\n",
    "\n",
    "We now have an xarray dataset with 3 coordinates: year, x and y, as well as the data variables returned by the evalscript as data variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6545a8c",
   "metadata": {},
   "source": [
    "We can also similarly plot the NDVI values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ccd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2.NDVI.plot(cmap=\"PRGn\", x=\"x\", y=\"y\", col=\"year\", col_wrap=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d96a4",
   "metadata": {},
   "source": [
    "![https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-16-output-1.png](https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-16-output-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2075f",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "For analysis the first step is to classify pixels as forest. In our case we will just do a simple thresholding classification where we classify everything above a certain threshold as forest. This isn’t the best approach for classifying forest, since agricultural areas can also easily reach very high NDVI values. A better approach would be to classify based on the temporal signature of the pixel.\n",
    "\n",
    "However, for this basic analysis, we stick to the simple thresholding approach.\n",
    "\n",
    "In this case we classify everything above an NDVI of 0.7 as forest. This calculated forest mask is then saved to a new Data Variable in the xarray dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e46007",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2[\"FOREST\"] = ds_s2.NDVI > 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034205fc",
   "metadata": {},
   "source": [
    "With this forest mask we can already do a quick preliminary analysis to plot the total forest area over the years.\n",
    "\n",
    "To do this we sum up the pixels along the x and y coordinate but not along the time coordinate. This will leave us with one value per year representing the number of pixels classified as forest. We can then calculate the forest area by multiplying the number of forest pixels by the resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8da179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_km2(dataarray, resolution):\n",
    "    # Calculate forest area\n",
    "    return dataarray * np.prod(list(resolution)) / 1e6\n",
    "\n",
    "\n",
    "forest_pixels = ds_s2.FOREST.sum([\"x\", \"y\"])\n",
    "forest_area_km2 = to_km2(forest_pixels, resolution)\n",
    "forest_area_km2.plot()\n",
    "plt.title(\"Forest Cover\")\n",
    "plt.ylabel(\"Forest Cover [km²]\")\n",
    "plt.ylim(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf7c21",
   "metadata": {},
   "source": [
    "![https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-18-output-1.png](https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-18-output-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb396f",
   "metadata": {},
   "source": [
    "We can see that the total forest area in this AOI decreased from around 80 km² in 2018 to only around 50 km² in 2023.\n",
    "\n",
    "The next step is to make change maps from year to year. To do this we basically take the difference of the forest mask of a year with its previous year.\n",
    "\n",
    "This will result in 0 value where there has been no change, -1 where forest was lost and +1 where forest was gained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62975990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make change maps of forest loss and forest gain compared to previous year\n",
    "\n",
    "# 0 - 0 = No Change: 0\n",
    "# 1 - 1 = No Change: 0\n",
    "# 1 - 0 = Forest Gain: 1\n",
    "# 0 - 1 = Forest Loss: -1\n",
    "\n",
    "# Define custom colors and labels\n",
    "colors = [\"darkred\", \"white\", \"darkblue\"]\n",
    "labels = [\"Forest Loss\", \"No Change\", \"Forest Gain\"]\n",
    "\n",
    "# Create a colormap and normalize it\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "norm = plt.Normalize(-1, 1)  # Adjust the range based on your data\n",
    "\n",
    "plot_year = 2022\n",
    "ds_s2[\"CHANGE\"] = ds_s2.FOREST.astype(int).diff(\"year\", label=\"upper\")\n",
    "ds_s2.CHANGE.sel(year=plot_year).plot(cmap=cmap, norm=norm, add_colorbar=False)\n",
    "\n",
    "# Create a legend with string labels\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=color, label=label) for color, label in zip(colors, labels)\n",
    "]\n",
    "plt.legend(handles=legend_patches, loc=\"lower left\")\n",
    "plt.title(f\"Forest Change Map {plot_year}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7e173",
   "metadata": {},
   "source": [
    "![https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-19-output-1.png](https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-19-output-1.png)\n",
    "\n",
    "Here, we can see the spatial distribution of areas affected by forest loss. In the displayed change from 2021 to 2022, most of the forest loss happened in the northern part of the study area, while the southern part lost comparatively less forest.\n",
    "\n",
    "To get a feel for the loss per year, we can cumulatively sum up the lost areas over the years. This should basically follow the same trends as the earlier plot of total forest area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eebd1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest Loss per Year\n",
    "forest_loss = (ds_s2.CHANGE == -1).sum([\"x\", \"y\"])\n",
    "forest_loss_km2 = to_km2(forest_loss, resolution)\n",
    "forest_loss_km2.cumsum().plot()\n",
    "plt.title(\"Cumulative Forest Loss\")\n",
    "plt.ylabel(\"Forest Loss [km²]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9c405",
   "metadata": {},
   "source": [
    "![https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-20-output-1.png](https://documentation.dataspace.copernicus.eu/notebook-samples/sentinelhub/deforestation_monitoring_with_xarray_files/figure-html/cell-20-output-1.png)\n",
    "\n",
    "We can see that there have been two years with particularly large amounts of lost forest area. From 2019-2020 and with by far the most lost area between 2021 and 2022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
